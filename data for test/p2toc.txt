Abstract 1
1 Introduction 2
2 Background 2
3 Model Architecture 2
3.1 Encoder and Decoder Stacks 3
3.2 Attention 3
3.2.1 Scaled Dot-Product Attention 4
3.2.2 Multi-Head Attention 4
3.2.3 Applications of Attention in our Model 5
3.3 Position-wise Feed-Forward Networks 5
3.4 Embeddings and Softmax 5
3.5 Positional Encoding 6
4 Why Self-Attention 6
5 Training 7
5.1 Training Data and Batching 7
5.2 Hardware and Schedule 7
5.3 Optimizer 7
5.4 Regularization 7
6 Results 8
6.1 Machine Translation 8
6.2 Model Variations 8
6.3 English Constituency Parsing 9
7 Conclusion 10
References 10